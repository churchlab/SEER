{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import distance\n",
    "from fuzzysearch import find_near_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lib_Info = pd.read_csv('~/Dropbox/xenoMAGE/Libraries/Lib_Info.csv',index_col=2)\n",
    "barcode_to_protein = dict(zip(Lib_Info.Barcode,Lib_Info.Protein))\n",
    "protein_to_family = dict(zip(Lib_Info.Protein,Lib_Info.Family))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some initial data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/20/18 - TW14 - Initial read of SSAP and SSAP/SSB libraries in pET-DEST vectors along with early rounds of selection in EC and LL. \n",
    "  \n",
    "Ran v3 150 with only F read (150bp). Got ~9.3 M reads (underclustered). The library was amplified with primers that bind before the barcode, and standard reverse primers for the plasmid. Amplicons were short for pJP005 and pARC8, but significantly longer for p444 as I only had long-distance primers.\n",
    "\n",
    "Samples 42 - 69 = SSAP lib in pARC8 selections 1 to 6  \n",
    "Samples 86,87,88 = initial SSAP libraries in p444-DEST, pJP005-DEST, and pARC8-DEST in host organisms  \n",
    "\n",
    "\n",
    "# 9/10/18 - TW15 - Finished EC SSAP/Dual selections, LL SSAP selection, more rounds on LL Dual, a few initial rounds on myco.\n",
    "  \n",
    "Ran v3 150 with only F read (150bp). Got ~15.4 M reads (underclustered). The library was amplified with primers that bind before the barcode, and standard reverse primers for the plasmid. Amplicons were short for pJP005 and pARC8, but significantly longer for p444 as I only had long-distance primers.\n",
    "\n",
    "Samples 33 -  48 = SSAP lib in pARC8 selections 7 to 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sample 0 ..\n",
      "took  0.5532839999999997 seconds to process 0 which contains: \n",
      " 21752 forward reads \n",
      " 109 of which were binned as junk, and \n",
      " 2472 of which had no match.\n",
      "processing sample 1_1 ..\n",
      "took  1.099501 seconds to process 1_1 which contains: \n",
      " 41870 forward reads \n",
      " 228 of which were binned as junk, and \n",
      " 971 of which had no match.\n",
      "processing sample 1_2 ..\n",
      "took  0.1686230000000002 seconds to process 1_2 which contains: \n",
      " 6441 forward reads \n",
      " 420 of which were binned as junk, and \n",
      " 268 of which had no match.\n",
      "processing sample 1_3 ..\n",
      "took  0.8291700000000004 seconds to process 1_3 which contains: \n",
      " 32296 forward reads \n",
      " 213 of which were binned as junk, and \n",
      " 762 of which had no match.\n",
      "processing sample 1_- ..\n",
      "took  1.2615570000000007 seconds to process 1_- which contains: \n",
      " 49491 forward reads \n",
      " 336 of which were binned as junk, and \n",
      " 6155 of which had no match.\n",
      "processing sample 2_1 ..\n",
      "took  0.8513390000000003 seconds to process 2_1 which contains: \n",
      " 33147 forward reads \n",
      " 177 of which were binned as junk, and \n",
      " 769 of which had no match.\n",
      "processing sample 2_2 ..\n",
      "took  0.562568999999999 seconds to process 2_2 which contains: \n",
      " 22196 forward reads \n",
      " 86 of which were binned as junk, and \n",
      " 481 of which had no match.\n",
      "processing sample 2_3 ..\n",
      "took  0.6333749999999991 seconds to process 2_3 which contains: \n",
      " 25197 forward reads \n",
      " 131 of which were binned as junk, and \n",
      " 748 of which had no match.\n",
      "processing sample 2_- ..\n",
      "took  1.1863969999999995 seconds to process 2_- which contains: \n",
      " 47477 forward reads \n",
      " 300 of which were binned as junk, and \n",
      " 1589 of which had no match.\n",
      "processing sample 3_1 ..\n",
      "took  1.1827740000000002 seconds to process 3_1 which contains: \n",
      " 45424 forward reads \n",
      " 216 of which were binned as junk, and \n",
      " 5748 of which had no match.\n",
      "processing sample 3_2 ..\n",
      "took  0.7561790000000013 seconds to process 3_2 which contains: \n",
      " 30121 forward reads \n",
      " 175 of which were binned as junk, and \n",
      " 632 of which had no match.\n",
      "processing sample 3_3 ..\n",
      "took  1.3651309999999999 seconds to process 3_3 which contains: \n",
      " 54097 forward reads \n",
      " 272 of which were binned as junk, and \n",
      " 1124 of which had no match.\n",
      "processing sample 3_- ..\n",
      "took  0.6453370000000014 seconds to process 3_- which contains: \n",
      " 24502 forward reads \n",
      " 127 of which were binned as junk, and \n",
      " 705 of which had no match.\n",
      "processing sample 4_1 ..\n",
      "took  1.14893 seconds to process 4_1 which contains: \n",
      " 43541 forward reads \n",
      " 255 of which were binned as junk, and \n",
      " 1177 of which had no match.\n",
      "processing sample 4_2 ..\n",
      "took  1.9284779999999984 seconds to process 4_2 which contains: \n",
      " 72647 forward reads \n",
      " 525 of which were binned as junk, and \n",
      " 8838 of which had no match.\n",
      "processing sample 4_3 ..\n",
      "took  1.1032110000000017 seconds to process 4_3 which contains: \n",
      " 41458 forward reads \n",
      " 283 of which were binned as junk, and \n",
      " 934 of which had no match.\n",
      "processing sample 4_- ..\n",
      "took  1.082651000000002 seconds to process 4_- which contains: \n",
      " 40933 forward reads \n",
      " 298 of which were binned as junk, and \n",
      " 1129 of which had no match.\n",
      "processing sample 5_1 ..\n",
      "took  1.550694 seconds to process 5_1 which contains: \n",
      " 59861 forward reads \n",
      " 320 of which were binned as junk, and \n",
      " 1783 of which had no match.\n",
      "processing sample 5_2 ..\n",
      "took  0.9350370000000012 seconds to process 5_2 which contains: \n",
      " 36167 forward reads \n",
      " 175 of which were binned as junk, and \n",
      " 840 of which had no match.\n",
      "processing sample 5_3 ..\n",
      "took  0.8996450000000031 seconds to process 5_3 which contains: \n",
      " 33982 forward reads \n",
      " 144 of which were binned as junk, and \n",
      " 3953 of which had no match.\n",
      "processing sample 5_- ..\n",
      "took  0.5124980000000008 seconds to process 5_- which contains: \n",
      " 18898 forward reads \n",
      " 144 of which were binned as junk, and \n",
      " 544 of which had no match.\n",
      "processing sample 5p_1 ..\n",
      "took  0.3808599999999984 seconds to process 5p_1 which contains: \n",
      " 14873 forward reads \n",
      " 106 of which were binned as junk, and \n",
      " 352 of which had no match.\n",
      "processing sample 5p_2 ..\n",
      "took  0.13819099999999906 seconds to process 5p_2 which contains: \n",
      " 5653 forward reads \n",
      " 57 of which were binned as junk, and \n",
      " 232 of which had no match.\n",
      "processing sample 5p_3 ..\n",
      "took  0.13665000000000305 seconds to process 5p_3 which contains: \n",
      " 5365 forward reads \n",
      " 40 of which were binned as junk, and \n",
      " 154 of which had no match.\n",
      "processing sample 5p_- ..\n",
      "took  0.47041499999999914 seconds to process 5p_- which contains: \n",
      " 17626 forward reads \n",
      " 80 of which were binned as junk, and \n",
      " 2143 of which had no match.\n",
      "processing sample 6_1 ..\n",
      "took  0.4105880000000006 seconds to process 6_1 which contains: \n",
      " 15420 forward reads \n",
      " 92 of which were binned as junk, and \n",
      " 546 of which had no match.\n",
      "processing sample 6_2 ..\n",
      "took  1.0670959999999994 seconds to process 6_2 which contains: \n",
      " 41206 forward reads \n",
      " 621 of which were binned as junk, and \n",
      " 1207 of which had no match.\n",
      "processing sample 6_3 ..\n",
      "took  0.5838409999999996 seconds to process 6_3 which contains: \n",
      " 21926 forward reads \n",
      " 214 of which were binned as junk, and \n",
      " 679 of which had no match.\n",
      "processing sample 6_- ..\n",
      "took  0.27534800000000104 seconds to process 6_- which contains: \n",
      " 10166 forward reads \n",
      " 173 of which were binned as junk, and \n",
      " 365 of which had no match.\n",
      "processing sample 7_1 ..\n",
      "took  0.8349030000000006 seconds to process 7_1 which contains: \n",
      " 32514 forward reads \n",
      " 357 of which were binned as junk, and \n",
      " 819 of which had no match.\n",
      "processing sample 7_2 ..\n",
      "took  1.1260140000000014 seconds to process 7_2 which contains: \n",
      " 45652 forward reads \n",
      " 384 of which were binned as junk, and \n",
      " 1975 of which had no match.\n",
      "processing sample 7_3 ..\n",
      "took  0.6818619999999989 seconds to process 7_3 which contains: \n",
      " 26862 forward reads \n",
      " 252 of which were binned as junk, and \n",
      " 1459 of which had no match.\n",
      "processing sample 7_- ..\n",
      "took  0.3422370000000008 seconds to process 7_- which contains: \n",
      " 13352 forward reads \n",
      " 130 of which were binned as junk, and \n",
      " 268 of which had no match.\n",
      "processing sample 8_1 ..\n",
      "took  0.11172699999999836 seconds to process 8_1 which contains: \n",
      " 4228 forward reads \n",
      " 38 of which were binned as junk, and \n",
      " 123 of which had no match.\n",
      "processing sample 8_2 ..\n",
      "took  0.17598100000000017 seconds to process 8_2 which contains: \n",
      " 6590 forward reads \n",
      " 70 of which were binned as junk, and \n",
      " 164 of which had no match.\n",
      "processing sample 8_3 ..\n",
      "took  0.21910400000000152 seconds to process 8_3 which contains: \n",
      " 8667 forward reads \n",
      " 90 of which were binned as junk, and \n",
      " 265 of which had no match.\n",
      "processing sample 8_- ..\n",
      "took  0.2074220000000011 seconds to process 8_- which contains: \n",
      " 7651 forward reads \n",
      " 68 of which were binned as junk, and \n",
      " 213 of which had no match.\n",
      "processing sample 9_1 ..\n",
      "took  0.3459019999999988 seconds to process 9_1 which contains: \n",
      " 12831 forward reads \n",
      " 117 of which were binned as junk, and \n",
      " 412 of which had no match.\n",
      "processing sample 9_2 ..\n",
      "took  0.29734000000000194 seconds to process 9_2 which contains: \n",
      " 10776 forward reads \n",
      " 128 of which were binned as junk, and \n",
      " 266 of which had no match.\n",
      "processing sample 9_3 ..\n",
      "took  0.6226389999999995 seconds to process 9_3 which contains: \n",
      " 23088 forward reads \n",
      " 244 of which were binned as junk, and \n",
      " 549 of which had no match.\n",
      "processing sample 9_- ..\n",
      "took  0.696667999999999 seconds to process 9_- which contains: \n",
      " 27124 forward reads \n",
      " 211 of which were binned as junk, and \n",
      " 644 of which had no match.\n",
      "processing sample 10_1 ..\n",
      "took  1.0461529999999968 seconds to process 10_1 which contains: \n",
      " 39548 forward reads \n",
      " 355 of which were binned as junk, and \n",
      " 1073 of which had no match.\n",
      "processing sample 10_2 ..\n",
      "took  0.7970230000000029 seconds to process 10_2 which contains: \n",
      " 30633 forward reads \n",
      " 278 of which were binned as junk, and \n",
      " 1761 of which had no match.\n",
      "processing sample 10_3 ..\n",
      "took  0.8189649999999986 seconds to process 10_3 which contains: \n",
      " 31123 forward reads \n",
      " 260 of which were binned as junk, and \n",
      " 1498 of which had no match.\n",
      "processing sample 10_- ..\n",
      "took  0.5652509999999964 seconds to process 10_- which contains: \n",
      " 21929 forward reads \n",
      " 141 of which were binned as junk, and \n",
      " 454 of which had no match.\n"
     ]
    }
   ],
   "source": [
    "steplist = ['0','1','2','3','4','5','5p','6','7','8','9','10']\n",
    "\n",
    "tw14Initial = [88]\n",
    "tw14 = [42,69]\n",
    "tw15 = [33,48]\n",
    "\n",
    "replicates = ['1','2','3','-']\n",
    "preSequence = 'CTTCCGATCTC'\n",
    "editing = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "# Find files in TW14 folder which have been merged from multiple sequencing runs and create list\n",
    "merge_list = []\n",
    "files = os.listdir('./TW14')  \n",
    "pattern = \"*merge.fastq\"  \n",
    "for entry in files:  \n",
    "    if fnmatch.fnmatch(entry, pattern):\n",
    "        merge_list.append(int(re.findall('\\d+',entry)[1]))\n",
    "\n",
    "barcodes = {}\n",
    "barcodes2 = {}\n",
    "barcode_pairs = {}\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "samples = [(m,14) for m in tw14Initial]\n",
    "samples += [(m,14) for m in range(tw14[0],tw14[1]+1)]\n",
    "samples += [(m,15) for m in range(tw15[0],tw15[1]+1)]\n",
    "\n",
    "\n",
    "for i in samples:\n",
    "    if a < 1:\n",
    "        sample_name = steplist[a]\n",
    "    else:\n",
    "        sample_name = steplist[a] + '_' + replicates[b]\n",
    "\n",
    "    # clock the process time for each sample\n",
    "    print('processing sample',sample_name,'..')\n",
    "    t0 = time.clock()\n",
    "\n",
    "    # initialize tracking variables\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    no_match = 0\n",
    "\n",
    "    # read-in fastQ files using BioPython\n",
    "    if i[1] == 14:\n",
    "        if i[0] in merge_list:\n",
    "            file = 'TW14/trimmed_TW14-%s_R1_merge.fastq' % str(i[0]).zfill(2)\n",
    "        else:\n",
    "            file = 'TW14/trimmed_TW14-%s_R1.fastq' % str(i[0]).zfill(2)\n",
    "    elif i[1] == 15:\n",
    "        file = 'TW15/trimmed_TW15-%s_R1.fastq' % str(i[0]).zfill(2)\n",
    "\n",
    "    trimmed = SeqIO.parse(file,'fastq')\n",
    "\n",
    "    # initialize tracking variable\n",
    "    barcodes[sample_name] = []\n",
    "\n",
    "    # iterate through reads\n",
    "    for j in trimmed:\n",
    "        count += 1\n",
    "        match = ''\n",
    "        match2 = ''\n",
    "\n",
    "        # quality filter any reads shorter than 50-nt\n",
    "        seq = str(j.seq)\n",
    "        seqlen = len(seq)\n",
    "        if seqlen > 60 and not re.search('[^ACTG]',seq):\n",
    "\n",
    "            # match to an 11-nt sequence 1 bp before the beginning of the varied region.\n",
    "            match = seq.find(preSequence)\n",
    "\n",
    "            # if the priming sequence(s) are found, record barcode(s), otherwise count as junk.\n",
    "            if match > 0:\n",
    "                barcode = seq[match+12 : match+24]\n",
    "                barcodes[sample_name].append(barcode)\n",
    "            else:\n",
    "                no_match += 1\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "    # report out time and statistics\n",
    "    t1 = time.clock()\n",
    "    print('took ',t1-t0,'seconds to process',sample_name,'which contains: \\n',count,'forward reads',\n",
    "          '\\n',junk,'of which were binned as junk, and \\n',no_match,'of which had no match.')\n",
    "\n",
    "    # iterate counter\n",
    "    b += 1         \n",
    "    if a == 0:\n",
    "        a = 1\n",
    "        b = 0\n",
    "    if a == -1:\n",
    "        a = 0\n",
    "        b = 0\n",
    "    if b == len(replicates):\n",
    "        b = 0\n",
    "        a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing SEER members:\n",
      " ['SR073', 'SR125']\n"
     ]
    }
   ],
   "source": [
    "# combine barcodes into dataframe, count their occurrences, separate data by experiment, sort and filter data,\n",
    "# and export data as an excel file.\n",
    "\n",
    "n = 35 # filter out any single barcodes counted fewer than n times in aggregate\n",
    "\n",
    "# First count the number of times each barcode is seen in every experiment\n",
    "\n",
    "barcodes = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in barcodes.items() ]))\n",
    "counts = pd.DataFrame()\n",
    "for c in barcodes.columns:\n",
    "    count = barcodes[c].value_counts()\n",
    "    count = pd.DataFrame(count,columns=[c])\n",
    "    counts = pd.concat([counts,count], axis=1)\n",
    "\n",
    "sums = counts.sum(axis=1)\n",
    "sums = sums[sums > n]\n",
    "counts = counts[counts.index.isin(sums.index)]\n",
    "\n",
    "# Then assign a library number to each barcode and sort dataframes by library number and then the initial count\n",
    "counts.insert(loc=0, column='SSAP', value=[None]*counts.shape[0])\n",
    "for i in counts.index.tolist():\n",
    "    try:\n",
    "        counts.loc[i,'SSAP'] = barcode_to_protein[i]\n",
    "    except:\n",
    "        continue\n",
    "counts = counts.sort_values(by=['SSAP','0'],ascending=[True,False])\n",
    "\n",
    "# Search unmatched barcodes to see if they are one mutation away from only one known barcode\n",
    "# Add unmatched count to known barcode's count in that case. Otherwise drop unmatched barcode.\n",
    "\n",
    "for mutant in counts[counts.SSAP.isnull()].index:\n",
    "    neighbors = []\n",
    "    for barcode in barcode_to_protein.keys():\n",
    "        if distance.levenshtein(mutant,barcode) == 1:\n",
    "            neighbors.append(barcode)\n",
    "    if len(neighbors) == 1:\n",
    "        new_count = counts.loc[neighbors[0]].drop('SSAP').fillna(0) + counts.loc[mutant].drop('SSAP').fillna(0)\n",
    "        new_count['SSAP'] = counts.loc[neighbors[0]].SSAP\n",
    "        counts.loc[neighbors[0]] = new_count\n",
    "    counts = counts.drop(mutant)\n",
    "\n",
    "# Remove non-SEER members\n",
    "\n",
    "counts = counts.set_index('SSAP').loc[[x for x in counts.SSAP if x[0:2] == 'SR']]\n",
    "\n",
    "# Then assign family to each SSAP\n",
    "counts.insert(loc=0, column='Family', value=[None]*counts.shape[0])\n",
    "for i in counts.index.tolist():\n",
    "    try:\n",
    "        counts.loc[i,'Family'] = protein_to_family[i]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Report out missing members\n",
    "\n",
    "print('missing SEER members:\\n',\n",
    "      [y for y in [x for x in barcode_to_protein.values() if x[:2] == 'SR'] if y not in counts.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selections = 10\n",
    "replicates = 3\n",
    "# Minimum frequency in negative control for downstream analylsis\n",
    "minimum = 0.0005\n",
    "\n",
    "for sel in range(1,selections+1):\n",
    "    df = pd.DataFrame()\n",
    "    dg = pd.DataFrame()\n",
    "    \n",
    "    negative = str(sel) + '_-'\n",
    "    dg['neg_frequency'] = counts[negative] / counts[negative].sum()\n",
    "    dg['threshold'] = dg['neg_frequency'].apply(lambda x: 1 if x >= minimum else 0)\n",
    "    \n",
    "    # Calculate enrichment relative to non-selective control\n",
    "    for rep in range(1,replicates+1):\n",
    "        handle = str(sel) + '_' + str(rep)\n",
    "        df[handle] = counts[handle] / counts[handle].sum()\n",
    "    \n",
    "    # Record and remove enrichment values from analysis for those with low denominator in negative run.\n",
    "    counts['enrichment_'+str(sel)] = df.sum(axis=1) / dg['neg_frequency'] * dg.threshold\n",
    "    \n",
    "    # Record average frequency values for selective replicates\n",
    "    counts['frequency_'+str(sel)] = df.sum(axis=1) / 3\n",
    "\n",
    "# Re-order columns\n",
    "counts = counts.sort_index(axis=1)\n",
    "\n",
    "#Write out to Excel\n",
    "writer = pd.ExcelWriter('SR016.xlsx', engine='xlsxwriter')\n",
    "counts.sort_values('enrichment_10',ascending=False).to_excel(writer, sheet_name = 'SEER')\n",
    "counts.groupby(['Family']).sum().to_excel(writer, sheet_name = 'Family Stats')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiencies of best members...TW20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3 seconds process time for loop PF020 - 1 with 48126 reads of which 2891 are junk.\n",
      "12.6 seconds process time for loop SR011 - 1 with 111170 reads of which 3624 are junk.\n",
      "14.4 seconds process time for loop SR016 - 1 with 128740 reads of which 12357 are junk.\n",
      "7.4 seconds process time for loop SR055 - 1 with 64494 reads of which 6204 are junk.\n",
      "2.8 seconds process time for loop SR085 - 1 with 25237 reads of which 2071 are junk.\n",
      "5.1 seconds process time for loop SR095 - 1 with 44676 reads of which 2568 are junk.\n",
      "2.8 seconds process time for loop SR120 - 1 with 23966 reads of which 336 are junk.\n",
      "4.8 seconds process time for loop PF020 - 2 with 42087 reads of which 3293 are junk.\n",
      "15.2 seconds process time for loop SR011 - 2 with 143240 reads of which 19829 are junk.\n",
      "24.9 seconds process time for loop SR016 - 2 with 219928 reads of which 10955 are junk.\n",
      "13.9 seconds process time for loop SR055 - 2 with 123317 reads of which 7715 are junk.\n",
      "15.8 seconds process time for loop SR085 - 2 with 136497 reads of which 5495 are junk.\n",
      "16.3 seconds process time for loop SR095 - 2 with 149327 reads of which 21404 are junk.\n",
      "14.2 seconds process time for loop SR120 - 2 with 130614 reads of which 19902 are junk.\n",
      "14.8 seconds process time for loop PF020 - 3 with 131973 reads of which 11230 are junk.\n",
      "7.6 seconds process time for loop SR011 - 3 with 65458 reads of which 2578 are junk.\n",
      "9.3 seconds process time for loop SR016 - 3 with 79136 reads of which 1553 are junk.\n",
      "6.2 seconds process time for loop SR055 - 3 with 54944 reads of which 4109 are junk.\n",
      "7.1 seconds process time for loop SR085 - 3 with 65017 reads of which 7511 are junk.\n",
      "2.9 seconds process time for loop SR095 - 3 with 25026 reads of which 1216 are junk.\n",
      "7.3 seconds process time for loop SR120 - 3 with 65558 reads of which 3436 are junk.\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['sample','replicate','codon_counts'])\n",
    "file = 'TW20/trimmed_TW20-%s_R1.fastq'\n",
    "SSAPs = ['PF020','SR011','SR016','SR055','SR085','SR095','SR120']\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "\n",
    "s = 0\n",
    "rep = 1\n",
    "for i in range(17,38):\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % (str(i).zfill(2))\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [SSAPs[s],rep,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',SSAPs[s],'-',rep,'with',count,'reads',\n",
    "          'of which',junk,'are junk.')\n",
    "\n",
    "    s += 1\n",
    "    if s == 7:\n",
    "        s = 0\n",
    "        rep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample PF020 \n",
      "positive:  8311 \n",
      "negative:  36687 \n",
      "junk:  238 \n",
      "efficiency:  0.184697097649 \n",
      "junk rate:  0.00526129631267 \n",
      "\n",
      "sample PF020 \n",
      "positive:  5887 \n",
      "negative:  32743 \n",
      "junk:  165 \n",
      "efficiency:  0.152394512037 \n",
      "junk rate:  0.00425312540276 \n",
      "\n",
      "sample PF020 \n",
      "positive:  18406 \n",
      "negative:  101724 \n",
      "junk:  614 \n",
      "efficiency:  0.153217347873 \n",
      "junk rate:  0.00508513880607 \n",
      "\n",
      "sample SR011 \n",
      "positive:  24853 \n",
      "negative:  82189 \n",
      "junk:  505 \n",
      "efficiency:  0.232179892005 \n",
      "junk rate:  0.00469562144923 \n",
      "\n",
      "sample SR011 \n",
      "positive:  21989 \n",
      "negative:  100662 \n",
      "junk:  761 \n",
      "efficiency:  0.179281049482 \n",
      "junk rate:  0.00616633714712 \n",
      "\n",
      "sample SR011 \n",
      "positive:  13473 \n",
      "negative:  49100 \n",
      "junk:  308 \n",
      "efficiency:  0.215316510316 \n",
      "junk rate:  0.00489814093287 \n",
      "\n",
      "sample SR016 \n",
      "positive:  34601 \n",
      "negative:  81005 \n",
      "junk:  778 \n",
      "efficiency:  0.299301074339 \n",
      "junk rate:  0.00668476766566 \n",
      "\n",
      "sample SR016 \n",
      "positive:  62059 \n",
      "negative:  145666 \n",
      "junk:  1249 \n",
      "efficiency:  0.298755566253 \n",
      "junk rate:  0.00597682008288 \n",
      "\n",
      "sample SR016 \n",
      "positive:  25314 \n",
      "negative:  51875 \n",
      "junk:  395 \n",
      "efficiency:  0.327948282786 \n",
      "junk rate:  0.00509125592906 \n",
      "\n",
      "sample SR055 \n",
      "positive:  843 \n",
      "negative:  57239 \n",
      "junk:  209 \n",
      "efficiency:  0.0145139630178 \n",
      "junk rate:  0.00358545916179 \n",
      "\n",
      "sample SR055 \n",
      "positive:  5087 \n",
      "negative:  110067 \n",
      "junk:  449 \n",
      "efficiency:  0.0441756256839 \n",
      "junk rate:  0.0038839822496 \n",
      "\n",
      "sample SR055 \n",
      "positive:  1180 \n",
      "negative:  49466 \n",
      "junk:  190 \n",
      "efficiency:  0.0232989772144 \n",
      "junk rate:  0.00373750885199 \n",
      "\n",
      "sample SR085 \n",
      "positive:  132 \n",
      "negative:  22950 \n",
      "junk:  85 \n",
      "efficiency:  0.00571874187679 \n",
      "junk rate:  0.00366901195666 \n",
      "\n",
      "sample SR085 \n",
      "positive:  496 \n",
      "negative:  130131 \n",
      "junk:  376 \n",
      "efficiency:  0.00379707104963 \n",
      "junk rate:  0.0028701632787 \n",
      "\n",
      "sample SR085 \n",
      "positive:  257 \n",
      "negative:  57071 \n",
      "junk:  179 \n",
      "efficiency:  0.00448297516048 \n",
      "junk rate:  0.00311266454519 \n",
      "\n",
      "sample SR095 \n",
      "positive:  10100 \n",
      "negative:  31725 \n",
      "junk:  284 \n",
      "efficiency:  0.241482367005 \n",
      "junk rate:  0.00674440143437 \n",
      "\n",
      "sample SR095 \n",
      "positive:  20374 \n",
      "negative:  106935 \n",
      "junk:  615 \n",
      "efficiency:  0.160035818363 \n",
      "junk rate:  0.00480754197805 \n",
      "\n",
      "sample SR095 \n",
      "positive:  5351 \n",
      "negative:  18349 \n",
      "junk:  111 \n",
      "efficiency:  0.225780590717 \n",
      "junk rate:  0.00466171097392 \n",
      "\n",
      "sample SR120 \n",
      "positive:  3571 \n",
      "negative:  19895 \n",
      "junk:  165 \n",
      "efficiency:  0.152177618682 \n",
      "junk rate:  0.00698235368795 \n",
      "\n",
      "sample SR120 \n",
      "positive:  14656 \n",
      "negative:  95576 \n",
      "junk:  481 \n",
      "efficiency:  0.132955947456 \n",
      "junk rate:  0.00434456658206 \n",
      "\n",
      "sample SR120 \n",
      "positive:  8209 \n",
      "negative:  53624 \n",
      "junk:  290 \n",
      "efficiency:  0.132760823508 \n",
      "junk rate:  0.00466815833105 \n",
      "\n",
      "replicate PF020 \n",
      "positive:  8311 \n",
      "negative:  36687 \n",
      "junk:  238 \n",
      "efficiency:  0.184697097649 \n",
      "junk rate:  0.00526129631267 \n",
      "\n",
      "replicate PF020 \n",
      "positive:  5887 \n",
      "negative:  32743 \n",
      "junk:  165 \n",
      "efficiency:  0.152394512037 \n",
      "junk rate:  0.00425312540276 \n",
      "\n",
      "replicate PF020 \n",
      "positive:  18406 \n",
      "negative:  101724 \n",
      "junk:  614 \n",
      "efficiency:  0.153217347873 \n",
      "junk rate:  0.00508513880607 \n",
      "\n",
      "replicate SR011 \n",
      "positive:  24853 \n",
      "negative:  82189 \n",
      "junk:  505 \n",
      "efficiency:  0.232179892005 \n",
      "junk rate:  0.00469562144923 \n",
      "\n",
      "replicate SR011 \n",
      "positive:  21989 \n",
      "negative:  100662 \n",
      "junk:  761 \n",
      "efficiency:  0.179281049482 \n",
      "junk rate:  0.00616633714712 \n",
      "\n",
      "replicate SR011 \n",
      "positive:  13473 \n",
      "negative:  49100 \n",
      "junk:  308 \n",
      "efficiency:  0.215316510316 \n",
      "junk rate:  0.00489814093287 \n",
      "\n",
      "replicate SR016 \n",
      "positive:  34601 \n",
      "negative:  81005 \n",
      "junk:  778 \n",
      "efficiency:  0.299301074339 \n",
      "junk rate:  0.00668476766566 \n",
      "\n",
      "replicate SR016 \n",
      "positive:  62059 \n",
      "negative:  145666 \n",
      "junk:  1249 \n",
      "efficiency:  0.298755566253 \n",
      "junk rate:  0.00597682008288 \n",
      "\n",
      "replicate SR016 \n",
      "positive:  25314 \n",
      "negative:  51875 \n",
      "junk:  395 \n",
      "efficiency:  0.327948282786 \n",
      "junk rate:  0.00509125592906 \n",
      "\n",
      "replicate SR055 \n",
      "positive:  843 \n",
      "negative:  57239 \n",
      "junk:  209 \n",
      "efficiency:  0.0145139630178 \n",
      "junk rate:  0.00358545916179 \n",
      "\n",
      "replicate SR055 \n",
      "positive:  5087 \n",
      "negative:  110067 \n",
      "junk:  449 \n",
      "efficiency:  0.0441756256839 \n",
      "junk rate:  0.0038839822496 \n",
      "\n",
      "replicate SR055 \n",
      "positive:  1180 \n",
      "negative:  49466 \n",
      "junk:  190 \n",
      "efficiency:  0.0232989772144 \n",
      "junk rate:  0.00373750885199 \n",
      "\n",
      "replicate SR085 \n",
      "positive:  132 \n",
      "negative:  22950 \n",
      "junk:  85 \n",
      "efficiency:  0.00571874187679 \n",
      "junk rate:  0.00366901195666 \n",
      "\n",
      "replicate SR085 \n",
      "positive:  496 \n",
      "negative:  130131 \n",
      "junk:  376 \n",
      "efficiency:  0.00379707104963 \n",
      "junk rate:  0.0028701632787 \n",
      "\n",
      "replicate SR085 \n",
      "positive:  257 \n",
      "negative:  57071 \n",
      "junk:  179 \n",
      "efficiency:  0.00448297516048 \n",
      "junk rate:  0.00311266454519 \n",
      "\n",
      "replicate SR095 \n",
      "positive:  10100 \n",
      "negative:  31725 \n",
      "junk:  284 \n",
      "efficiency:  0.241482367005 \n",
      "junk rate:  0.00674440143437 \n",
      "\n",
      "replicate SR095 \n",
      "positive:  20374 \n",
      "negative:  106935 \n",
      "junk:  615 \n",
      "efficiency:  0.160035818363 \n",
      "junk rate:  0.00480754197805 \n",
      "\n",
      "replicate SR095 \n",
      "positive:  5351 \n",
      "negative:  18349 \n",
      "junk:  111 \n",
      "efficiency:  0.225780590717 \n",
      "junk rate:  0.00466171097392 \n",
      "\n",
      "replicate SR120 \n",
      "positive:  3571 \n",
      "negative:  19895 \n",
      "junk:  165 \n",
      "efficiency:  0.152177618682 \n",
      "junk rate:  0.00698235368795 \n",
      "\n",
      "replicate SR120 \n",
      "positive:  14656 \n",
      "negative:  95576 \n",
      "junk:  481 \n",
      "efficiency:  0.132955947456 \n",
      "junk rate:  0.00434456658206 \n",
      "\n",
      "replicate SR120 \n",
      "positive:  8209 \n",
      "negative:  53624 \n",
      "junk:  290 \n",
      "efficiency:  0.132760823508 \n",
      "junk rate:  0.00466815833105 \n",
      "\n",
      "codon_counts PF020 \n",
      "positive:  8311 \n",
      "negative:  36687 \n",
      "junk:  238 \n",
      "efficiency:  0.184697097649 \n",
      "junk rate:  0.00526129631267 \n",
      "\n",
      "codon_counts PF020 \n",
      "positive:  5887 \n",
      "negative:  32743 \n",
      "junk:  165 \n",
      "efficiency:  0.152394512037 \n",
      "junk rate:  0.00425312540276 \n",
      "\n",
      "codon_counts PF020 \n",
      "positive:  18406 \n",
      "negative:  101724 \n",
      "junk:  614 \n",
      "efficiency:  0.153217347873 \n",
      "junk rate:  0.00508513880607 \n",
      "\n",
      "codon_counts SR011 \n",
      "positive:  24853 \n",
      "negative:  82189 \n",
      "junk:  505 \n",
      "efficiency:  0.232179892005 \n",
      "junk rate:  0.00469562144923 \n",
      "\n",
      "codon_counts SR011 \n",
      "positive:  21989 \n",
      "negative:  100662 \n",
      "junk:  761 \n",
      "efficiency:  0.179281049482 \n",
      "junk rate:  0.00616633714712 \n",
      "\n",
      "codon_counts SR011 \n",
      "positive:  13473 \n",
      "negative:  49100 \n",
      "junk:  308 \n",
      "efficiency:  0.215316510316 \n",
      "junk rate:  0.00489814093287 \n",
      "\n",
      "codon_counts SR016 \n",
      "positive:  34601 \n",
      "negative:  81005 \n",
      "junk:  778 \n",
      "efficiency:  0.299301074339 \n",
      "junk rate:  0.00668476766566 \n",
      "\n",
      "codon_counts SR016 \n",
      "positive:  62059 \n",
      "negative:  145666 \n",
      "junk:  1249 \n",
      "efficiency:  0.298755566253 \n",
      "junk rate:  0.00597682008288 \n",
      "\n",
      "codon_counts SR016 \n",
      "positive:  25314 \n",
      "negative:  51875 \n",
      "junk:  395 \n",
      "efficiency:  0.327948282786 \n",
      "junk rate:  0.00509125592906 \n",
      "\n",
      "codon_counts SR055 \n",
      "positive:  843 \n",
      "negative:  57239 \n",
      "junk:  209 \n",
      "efficiency:  0.0145139630178 \n",
      "junk rate:  0.00358545916179 \n",
      "\n",
      "codon_counts SR055 \n",
      "positive:  5087 \n",
      "negative:  110067 \n",
      "junk:  449 \n",
      "efficiency:  0.0441756256839 \n",
      "junk rate:  0.0038839822496 \n",
      "\n",
      "codon_counts SR055 \n",
      "positive:  1180 \n",
      "negative:  49466 \n",
      "junk:  190 \n",
      "efficiency:  0.0232989772144 \n",
      "junk rate:  0.00373750885199 \n",
      "\n",
      "codon_counts SR085 \n",
      "positive:  132 \n",
      "negative:  22950 \n",
      "junk:  85 \n",
      "efficiency:  0.00571874187679 \n",
      "junk rate:  0.00366901195666 \n",
      "\n",
      "codon_counts SR085 \n",
      "positive:  496 \n",
      "negative:  130131 \n",
      "junk:  376 \n",
      "efficiency:  0.00379707104963 \n",
      "junk rate:  0.0028701632787 \n",
      "\n",
      "codon_counts SR085 \n",
      "positive:  257 \n",
      "negative:  57071 \n",
      "junk:  179 \n",
      "efficiency:  0.00448297516048 \n",
      "junk rate:  0.00311266454519 \n",
      "\n",
      "codon_counts SR095 \n",
      "positive:  10100 \n",
      "negative:  31725 \n",
      "junk:  284 \n",
      "efficiency:  0.241482367005 \n",
      "junk rate:  0.00674440143437 \n",
      "\n",
      "codon_counts SR095 \n",
      "positive:  20374 \n",
      "negative:  106935 \n",
      "junk:  615 \n",
      "efficiency:  0.160035818363 \n",
      "junk rate:  0.00480754197805 \n",
      "\n",
      "codon_counts SR095 \n",
      "positive:  5351 \n",
      "negative:  18349 \n",
      "junk:  111 \n",
      "efficiency:  0.225780590717 \n",
      "junk rate:  0.00466171097392 \n",
      "\n",
      "codon_counts SR120 \n",
      "positive:  3571 \n",
      "negative:  19895 \n",
      "junk:  165 \n",
      "efficiency:  0.152177618682 \n",
      "junk rate:  0.00698235368795 \n",
      "\n",
      "codon_counts SR120 \n",
      "positive:  14656 \n",
      "negative:  95576 \n",
      "junk:  481 \n",
      "efficiency:  0.132955947456 \n",
      "junk rate:  0.00434456658206 \n",
      "\n",
      "codon_counts SR120 \n",
      "positive:  8209 \n",
      "negative:  53624 \n",
      "junk:  290 \n",
      "efficiency:  0.132760823508 \n",
      "junk rate:  0.00466815833105 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = pd.DataFrame(columns=['sample','replicate','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "for bug in tally.keys():\n",
    "    for sample in tally['sample'].unique():\n",
    "        data = tally[tally['sample'] == sample]\n",
    "        for replicate in data['replicate']:\n",
    "            for item in data[data['replicate'] == replicate].codon_counts:\n",
    "                positive = negative = junk = efficiency = junk_rate = 0\n",
    "                sample_name = sample + '_' + str(replicate)\n",
    "                for i in item.index:\n",
    "                    if i == editing_ref['negative']:\n",
    "                        negative = item[i]\n",
    "                    elif i == editing_ref['positive']:\n",
    "                        positive = item[i]\n",
    "                    else:\n",
    "                        junk += item[i]\n",
    "        \n",
    "                if negative > 0 :\n",
    "                    efficiency = positive/(positive+negative)\n",
    "                junk_rate = junk/(positive+negative+junk)\n",
    "                analysis.loc[sample_name] = [sample,replicate,positive,negative,junk,efficiency,junk_rate]\n",
    "                print(bug,sample,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,'\\nefficiency: ',efficiency,\n",
    "                      '\\njunk rate: ',junk_rate,'\\n')\n",
    "\n",
    "# write out to excel\n",
    "writer = pd.ExcelWriter('SR016-efficiencies.xlsx', engine='xlsxwriter')\n",
    "analysis.to_excel(writer, sheet_name = 'EC Winners')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta v. PapRecT v. PF020 efficiency tests with varying oligo and inducer conc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TW21: PapRecT v. Beta with varied arabinose\n",
    "  \n",
    "pARC8 RBS and original Beta codons (not SR085 codons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 seconds process time for loop SR016 - .01% - 1 with 82301 reads of which 14560 are junk.\n",
      "6.6 seconds process time for loop SR016 - .05% - 1 with 39615 reads of which 509 are junk.\n",
      "15.8 seconds process time for loop SR016 - .1% - 1 with 116199 reads of which 3745 are junk.\n",
      "6.1 seconds process time for loop SR016 - .2% - 1 with 48226 reads of which 2364 are junk.\n",
      "12.6 seconds process time for loop SR016 - .4% - 1 with 99954 reads of which 2848 are junk.\n",
      "10.8 seconds process time for loop Beta - .01% - 1 with 89099 reads of which 6342 are junk.\n",
      "6.9 seconds process time for loop Beta - .05% - 1 with 55148 reads of which 594 are junk.\n",
      "8.2 seconds process time for loop Beta - .1% - 1 with 66643 reads of which 1873 are junk.\n",
      "19.5 seconds process time for loop Beta - .2% - 1 with 158661 reads of which 4999 are junk.\n",
      "9.5 seconds process time for loop Beta - .4% - 1 with 78055 reads of which 2844 are junk.\n",
      "11.5 seconds process time for loop SR016 - .01% - 2 with 102558 reads of which 9704 are junk.\n",
      "8.6 seconds process time for loop SR016 - .05% - 2 with 64883 reads of which 1161 are junk.\n",
      "4.7 seconds process time for loop SR016 - .1% - 2 with 32049 reads of which 922 are junk.\n",
      "2.5 seconds process time for loop SR016 - .2% - 2 with 17047 reads of which 481 are junk.\n",
      "2.5 seconds process time for loop SR016 - .4% - 2 with 17885 reads of which 278 are junk.\n",
      "2.3 seconds process time for loop Beta - .01% - 2 with 16653 reads of which 544 are junk.\n",
      "7.8 seconds process time for loop Beta - .05% - 2 with 57170 reads of which 777 are junk.\n",
      "6.2 seconds process time for loop Beta - .1% - 2 with 46093 reads of which 1246 are junk.\n",
      "8.5 seconds process time for loop Beta - .2% - 2 with 63510 reads of which 2630 are junk.\n",
      "3.7 seconds process time for loop Beta - .4% - 2 with 27554 reads of which 671 are junk.\n",
      "6.5 seconds process time for loop SR016 - .01% - 3 with 45118 reads of which 3922 are junk.\n",
      "4.9 seconds process time for loop SR016 - .05% - 3 with 38386 reads of which 496 are junk.\n",
      "5.9 seconds process time for loop SR016 - .1% - 3 with 45265 reads of which 1541 are junk.\n",
      "4.8 seconds process time for loop SR016 - .2% - 3 with 35620 reads of which 1236 are junk.\n",
      "21.3 seconds process time for loop SR016 - .4% - 3 with 163411 reads of which 4758 are junk.\n",
      "14.6 seconds process time for loop Beta - .01% - 3 with 88655 reads of which 4556 are junk.\n",
      "12.6 seconds process time for loop Beta - .05% - 3 with 90289 reads of which 1241 are junk.\n",
      "12.3 seconds process time for loop Beta - .1% - 3 with 95318 reads of which 3759 are junk.\n",
      "23.9 seconds process time for loop Beta - .2% - 3 with 190401 reads of which 15590 are junk.\n",
      "19.1 seconds process time for loop Beta - .4% - 3 with 153001 reads of which 4555 are junk.\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['ssap','inducer','replicate','codon_counts'])\n",
    "file = 'TW21/trimmed_TW21-%s_R1.fastq'\n",
    "SSAPs = ['SR016','Beta']\n",
    "inducer = ['.01%','.05%','.1%','.2%','.4%']\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "\n",
    "s = 0\n",
    "i = 0\n",
    "rep = 1\n",
    "for j in range(1,31):\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % (str(j).zfill(2))\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [SSAPs[s],inducer[i],rep,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',SSAPs[s],'-',inducer[i],'-',\n",
    "          rep,'with',count,'reads','of which',junk,'are junk.')\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        i = 0\n",
    "        s += 1\n",
    "        if s == 2:\n",
    "            s = 0\n",
    "            rep += 1\n",
    "            \n",
    "analysis = pd.DataFrame(columns=['ssap','inducer','replicate','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "for ssap in tally['ssap'].unique():\n",
    "    for inducer in tally['inducer'].unique():\n",
    "        data = tally[(tally['ssap'] == ssap) & (tally['inducer'] == inducer)]\n",
    "        for replicate in data['replicate']:\n",
    "            for item in data[data['replicate'] == replicate].codon_counts:\n",
    "                positive = negative = junk = efficiency = junk_rate = 0\n",
    "                sample_name = ssap + '_' + inducer + '_' + str(replicate)\n",
    "                for i in item.index:\n",
    "                    if i == editing_ref['negative']:\n",
    "                        negative = item[i]\n",
    "                    elif i == editing_ref['positive']:\n",
    "                        positive = item[i]\n",
    "                    else:\n",
    "                        junk += item[i]\n",
    "\n",
    "                if negative > 0 :\n",
    "                    efficiency = positive/(positive+negative)\n",
    "                junk_rate = junk/(positive+negative+junk)\n",
    "                analysis.loc[sample_name] = [ssap,inducer,replicate,positive,negative,junk,efficiency,junk_rate]\n",
    "                print(sample_name,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,\n",
    "                      '\\nefficiency: ',efficiency,'\\njunk rate: ',junk_rate,'\\n')\n",
    "\n",
    "# write out to excel\n",
    "writer = pd.ExcelWriter('SR016-efficienciesb.xlsx', engine='xlsxwriter')\n",
    "analysis.to_excel(writer, sheet_name = 'SR016 v Beta')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TW22 - Beta v. PapRecT varying oligo conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 seconds process time for loop SR016 - 0.1 - 1 with 7507 reads of which 257 are junk.\n",
      "1.5 seconds process time for loop SR016 - 0.5 - 1 with 9772 reads of which 403 are junk.\n",
      "1.7 seconds process time for loop SR016 - 1 - 1 with 9470 reads of which 406 are junk.\n",
      "1.9 seconds process time for loop SR016 - 5 - 1 with 12988 reads of which 735 are junk.\n",
      "1.5 seconds process time for loop SR016 - 10 - 1 with 11224 reads of which 795 are junk.\n",
      "1.5 seconds process time for loop SR016 - 20 - 1 with 9522 reads of which 592 are junk.\n",
      "1.3 seconds process time for loop Beta - 0.1 - 1 with 10403 reads of which 457 are junk.\n",
      "1.2 seconds process time for loop Beta - 0.5 - 1 with 10072 reads of which 526 are junk.\n",
      "1.3 seconds process time for loop Beta - 1 - 1 with 10753 reads of which 394 are junk.\n",
      "1.2 seconds process time for loop Beta - 5 - 1 with 10473 reads of which 562 are junk.\n",
      "1.1 seconds process time for loop Beta - 10 - 1 with 9444 reads of which 462 are junk.\n",
      "1.0 seconds process time for loop Beta - 20 - 1 with 9145 reads of which 521 are junk.\n",
      "1.0 seconds process time for loop SR016 - 0.1 - 2 with 9298 reads of which 520 are junk.\n",
      "1.5 seconds process time for loop SR016 - 0.5 - 2 with 11995 reads of which 592 are junk.\n",
      "1.2 seconds process time for loop SR016 - 1 - 2 with 11051 reads of which 615 are junk.\n",
      "1.3 seconds process time for loop SR016 - 5 - 2 with 12065 reads of which 776 are junk.\n",
      "1.4 seconds process time for loop SR016 - 10 - 2 with 12941 reads of which 1116 are junk.\n",
      "1.2 seconds process time for loop SR016 - 20 - 2 with 11337 reads of which 601 are junk.\n",
      "1.4 seconds process time for loop Beta - 0.1 - 2 with 12518 reads of which 890 are junk.\n",
      "1.1 seconds process time for loop Beta - 0.5 - 2 with 9975 reads of which 513 are junk.\n",
      "1.2 seconds process time for loop Beta - 1 - 2 with 11161 reads of which 655 are junk.\n",
      "1.4 seconds process time for loop Beta - 5 - 2 with 12487 reads of which 790 are junk.\n",
      "1.2 seconds process time for loop Beta - 10 - 2 with 11180 reads of which 609 are junk.\n",
      "1.0 seconds process time for loop Beta - 20 - 2 with 8697 reads of which 773 are junk.\n",
      "0.9 seconds process time for loop SR016 - 0.1 - 3 with 8077 reads of which 286 are junk.\n",
      "1.3 seconds process time for loop SR016 - 0.5 - 3 with 9040 reads of which 373 are junk.\n",
      "1.4 seconds process time for loop SR016 - 1 - 3 with 10972 reads of which 482 are junk.\n",
      "1.2 seconds process time for loop SR016 - 5 - 3 with 11105 reads of which 427 are junk.\n",
      "1.2 seconds process time for loop SR016 - 10 - 3 with 10978 reads of which 460 are junk.\n",
      "1.1 seconds process time for loop SR016 - 20 - 3 with 9779 reads of which 369 are junk.\n",
      "1.3 seconds process time for loop Beta - 0.1 - 3 with 11594 reads of which 539 are junk.\n",
      "1.1 seconds process time for loop Beta - 0.5 - 3 with 9965 reads of which 455 are junk.\n",
      "1.0 seconds process time for loop Beta - 1 - 3 with 9088 reads of which 340 are junk.\n",
      "1.2 seconds process time for loop Beta - 5 - 3 with 10347 reads of which 411 are junk.\n",
      "1.3 seconds process time for loop Beta - 10 - 3 with 11663 reads of which 820 are junk.\n",
      "0.9 seconds process time for loop Beta - 20 - 3 with 8354 reads of which 609 are junk.\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['ssap','oligo','replicate','codon_counts'])\n",
    "file = 'TW22/trimmed_TW22-%s_R1.fastq'\n",
    "SSAPs = ['SR016','Beta']\n",
    "oligo = ['0.1','0.5','1','5','10','20']\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "\n",
    "s = 0\n",
    "o = 0\n",
    "rep = 1\n",
    "for j in range(1,37):\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % (str(j).zfill(2))\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [SSAPs[s],oligo[o],rep,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',SSAPs[s],'-',oligo[o],'-',\n",
    "          rep,'with',count,'reads','of which',junk,'are junk.')\n",
    "    o += 1\n",
    "    if o == 6:\n",
    "        o = 0\n",
    "        s += 1\n",
    "        if s == 2:\n",
    "            s = 0\n",
    "            rep += 1\n",
    "            \n",
    "analysis = pd.DataFrame(columns=['ssap','oligo','replicate','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "\n",
    "for ssap in tally['ssap'].unique():\n",
    "    for oligo in tally['oligo'].unique():\n",
    "        data = tally[(tally['ssap'] == ssap) & (tally['oligo'] == oligo)]\n",
    "        for replicate in data['replicate']:\n",
    "            for item in data[data['replicate'] == replicate].codon_counts:\n",
    "                positive = negative = junk = efficiency = junk_rate = 0\n",
    "                sample_name = ssap + '_' + oligo + '_' + str(replicate)\n",
    "                for i in item.index:\n",
    "                    if i == editing_ref['negative']:\n",
    "                        negative = item[i]\n",
    "                    elif i == editing_ref['positive']:\n",
    "                        positive = item[i]\n",
    "                    else:\n",
    "                        junk += item[i]\n",
    "\n",
    "                if negative > 0 :\n",
    "                    efficiency = positive/(positive+negative)\n",
    "                junk_rate = junk/(positive+negative+junk)\n",
    "                analysis.loc[sample_name] = [ssap,inducer,replicate,positive,negative,junk,efficiency,junk_rate]\n",
    "                print(sample_name,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,\n",
    "                      '\\nefficiency: ',efficiency,'\\njunk rate: ',junk_rate,'\\n')\n",
    "\n",
    "# write out to excel\n",
    "writer = pd.ExcelWriter('SR016-efficienciesc.xlsx', engine='xlsxwriter')\n",
    "analysis.to_excel(writer, sheet_name = 'SR016 v Beta')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TW25 - PF020 varied oligo and arabinose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 seconds process time for loop 97 with 8700 reads of which 701 are junk.\n",
      "1.2 seconds process time for loop 98 with 9243 reads of which 551 are junk.\n",
      "1.0 seconds process time for loop 99 with 8269 reads of which 572 are junk.\n",
      "1.1 seconds process time for loop 100 with 8543 reads of which 576 are junk.\n",
      "0.1 seconds process time for loop 101 with 973 reads of which 250 are junk.\n",
      "1.3 seconds process time for loop 102 with 10523 reads of which 598 are junk.\n",
      "1.3 seconds process time for loop 103 with 10827 reads of which 714 are junk.\n",
      "1.0 seconds process time for loop 104 with 7958 reads of which 611 are junk.\n",
      "1.1 seconds process time for loop 105 with 9033 reads of which 642 are junk.\n",
      "0.9 seconds process time for loop 106 with 7554 reads of which 488 are junk.\n",
      "1.0 seconds process time for loop 109 with 7751 reads of which 453 are junk.\n",
      "1.1 seconds process time for loop 110 with 8547 reads of which 541 are junk.\n",
      "1.3 seconds process time for loop 111 with 10649 reads of which 739 are junk.\n",
      "1.0 seconds process time for loop 112 with 7949 reads of which 455 are junk.\n",
      "1.0 seconds process time for loop 113 with 8472 reads of which 476 are junk.\n",
      "1.2 seconds process time for loop 114 with 9523 reads of which 580 are junk.\n",
      "1.0 seconds process time for loop 115 with 8230 reads of which 499 are junk.\n",
      "1.0 seconds process time for loop 116 with 7620 reads of which 469 are junk.\n",
      "1.3 seconds process time for loop 117 with 9452 reads of which 705 are junk.\n",
      "0.9 seconds process time for loop 118 with 6817 reads of which 398 are junk.\n",
      "1.4 seconds process time for loop 121 with 11077 reads of which 719 are junk.\n",
      "1.2 seconds process time for loop 122 with 9479 reads of which 658 are junk.\n",
      "1.5 seconds process time for loop 123 with 11930 reads of which 906 are junk.\n",
      "1.3 seconds process time for loop 124 with 10638 reads of which 800 are junk.\n",
      "0.2 seconds process time for loop 125 with 1894 reads of which 120 are junk.\n",
      "1.5 seconds process time for loop 126 with 11850 reads of which 846 are junk.\n",
      "1.5 seconds process time for loop 127 with 12154 reads of which 855 are junk.\n",
      "1.2 seconds process time for loop 128 with 9050 reads of which 556 are junk.\n",
      "1.3 seconds process time for loop 129 with 10888 reads of which 695 are junk.\n",
      "1.0 seconds process time for loop 130 with 7811 reads of which 459 are junk.\n",
      "97 \n",
      "positive:  2381 \n",
      "negative:  5378 \n",
      "junk:  241 \n",
      "efficiency:  0.3068694419383941 \n",
      "junk rate:  0.030125 \n",
      "\n",
      "98 \n",
      "positive:  1846 \n",
      "negative:  6572 \n",
      "junk:  275 \n",
      "efficiency:  0.2192919933475885 \n",
      "junk rate:  0.03163464856781318 \n",
      "\n",
      "99 \n",
      "positive:  1028 \n",
      "negative:  6397 \n",
      "junk:  273 \n",
      "efficiency:  0.13845117845117846 \n",
      "junk rate:  0.035463756819953236 \n",
      "\n",
      "100 \n",
      "positive:  427 \n",
      "negative:  7241 \n",
      "junk:  300 \n",
      "efficiency:  0.055685967657798645 \n",
      "junk rate:  0.03765060240963856 \n",
      "\n",
      "101 \n",
      "positive:  6 \n",
      "negative:  700 \n",
      "junk:  18 \n",
      "efficiency:  0.0084985835694051 \n",
      "junk rate:  0.024861878453038673 \n",
      "\n",
      "102 \n",
      "positive:  1623 \n",
      "negative:  8022 \n",
      "junk:  281 \n",
      "efficiency:  0.1682737169517885 \n",
      "junk rate:  0.028309490227684867 \n",
      "\n",
      "103 \n",
      "positive:  1630 \n",
      "negative:  8144 \n",
      "junk:  340 \n",
      "efficiency:  0.16676897892367507 \n",
      "junk rate:  0.03361676883527783 \n",
      "\n",
      "104 \n",
      "positive:  2053 \n",
      "negative:  5073 \n",
      "junk:  222 \n",
      "efficiency:  0.2880999158012911 \n",
      "junk rate:  0.03021230266739249 \n",
      "\n",
      "105 \n",
      "positive:  2470 \n",
      "negative:  5628 \n",
      "junk:  294 \n",
      "efficiency:  0.3050135836008891 \n",
      "junk rate:  0.03503336510962822 \n",
      "\n",
      "106 \n",
      "positive:  1507 \n",
      "negative:  5287 \n",
      "junk:  273 \n",
      "efficiency:  0.22181336473358845 \n",
      "junk rate:  0.03863025328993915 \n",
      "\n",
      "109 \n",
      "positive:  2149 \n",
      "negative:  4888 \n",
      "junk:  262 \n",
      "efficiency:  0.3053858178200938 \n",
      "junk rate:  0.035895328127140705 \n",
      "\n",
      "110 \n",
      "positive:  1775 \n",
      "negative:  5920 \n",
      "junk:  312 \n",
      "efficiency:  0.23066926575698504 \n",
      "junk rate:  0.038965904833270885 \n",
      "\n",
      "111 \n",
      "positive:  1199 \n",
      "negative:  8376 \n",
      "junk:  336 \n",
      "efficiency:  0.1252219321148825 \n",
      "junk rate:  0.03390172535566542 \n",
      "\n",
      "112 \n",
      "positive:  320 \n",
      "negative:  6922 \n",
      "junk:  253 \n",
      "efficiency:  0.04418668876001105 \n",
      "junk rate:  0.033755837224816544 \n",
      "\n",
      "113 \n",
      "positive:  58 \n",
      "negative:  7650 \n",
      "junk:  289 \n",
      "efficiency:  0.007524649714582252 \n",
      "junk rate:  0.03613855195698387 \n",
      "\n",
      "114 \n",
      "positive:  1804 \n",
      "negative:  6816 \n",
      "junk:  324 \n",
      "efficiency:  0.20928074245939676 \n",
      "junk rate:  0.03622540250447227 \n",
      "\n",
      "115 \n",
      "positive:  1755 \n",
      "negative:  5722 \n",
      "junk:  255 \n",
      "efficiency:  0.2347198074093888 \n",
      "junk rate:  0.032979824107604756 \n",
      "\n",
      "116 \n",
      "positive:  1465 \n",
      "negative:  5455 \n",
      "junk:  232 \n",
      "efficiency:  0.21170520231213874 \n",
      "junk rate:  0.03243847874720358 \n",
      "\n",
      "117 \n",
      "positive:  2484 \n",
      "negative:  5964 \n",
      "junk:  300 \n",
      "efficiency:  0.2940340909090909 \n",
      "junk rate:  0.03429355281207133 \n",
      "\n",
      "118 \n",
      "positive:  1694 \n",
      "negative:  4468 \n",
      "junk:  258 \n",
      "efficiency:  0.2749107432651736 \n",
      "junk rate:  0.04018691588785047 \n",
      "\n",
      "121 \n",
      "positive:  3640 \n",
      "negative:  6348 \n",
      "junk:  371 \n",
      "efficiency:  0.3644373247897477 \n",
      "junk rate:  0.035814267786465875 \n",
      "\n",
      "122 \n",
      "positive:  1971 \n",
      "negative:  6540 \n",
      "junk:  311 \n",
      "efficiency:  0.23158265773704617 \n",
      "junk rate:  0.035252777148039 \n",
      "\n",
      "123 \n",
      "positive:  1310 \n",
      "negative:  9360 \n",
      "junk:  355 \n",
      "efficiency:  0.12277413308341144 \n",
      "junk rate:  0.03219954648526077 \n",
      "\n",
      "124 \n",
      "positive:  527 \n",
      "negative:  9074 \n",
      "junk:  238 \n",
      "efficiency:  0.05489011561295699 \n",
      "junk rate:  0.024189450147372702 \n",
      "\n",
      "125 \n",
      "positive:  23 \n",
      "negative:  1699 \n",
      "junk:  53 \n",
      "efficiency:  0.013356562137049941 \n",
      "junk rate:  0.029859154929577466 \n",
      "\n",
      "126 \n",
      "positive:  2015 \n",
      "negative:  8614 \n",
      "junk:  376 \n",
      "efficiency:  0.18957568915231912 \n",
      "junk rate:  0.03416628805088596 \n",
      "\n",
      "127 \n",
      "positive:  1381 \n",
      "negative:  9603 \n",
      "junk:  316 \n",
      "efficiency:  0.12572833211944648 \n",
      "junk rate:  0.027964601769911505 \n",
      "\n",
      "128 \n",
      "positive:  1864 \n",
      "negative:  6340 \n",
      "junk:  291 \n",
      "efficiency:  0.227206240858118 \n",
      "junk rate:  0.0342554443790465 \n",
      "\n",
      "129 \n",
      "positive:  2313 \n",
      "negative:  7538 \n",
      "junk:  343 \n",
      "efficiency:  0.2347984976144554 \n",
      "junk rate:  0.03364724347655484 \n",
      "\n",
      "130 \n",
      "positive:  1639 \n",
      "negative:  5484 \n",
      "junk:  230 \n",
      "efficiency:  0.23009967710234452 \n",
      "junk rate:  0.0312797497620019 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['sample','codon_counts'])\n",
    "file = 'TW25/trimmed_TW25-%s_R1.fastq'\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "samples = [i for j in [range(97,107),range(109,119),range(121,131)] for i in j]\n",
    "\n",
    "for j in samples:\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % (str(j).zfill(2))\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [j,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',j,'with',count,'reads','of which',junk,'are junk.')\n",
    "\n",
    "analysis = pd.DataFrame(columns=['sample','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "for sample in tally['sample'].unique():\n",
    "    for item in tally[tally['sample'] == sample].codon_counts:\n",
    "        positive = negative = junk = efficiency = junk_rate = 0\n",
    "        \n",
    "        for i in item.index:\n",
    "            if i == editing_ref['negative']:\n",
    "                negative = item[i]\n",
    "            elif i == editing_ref['positive']:\n",
    "                positive = item[i]\n",
    "            else:\n",
    "                junk += item[i]\n",
    "\n",
    "        if negative > 0 :\n",
    "            efficiency = positive/(positive+negative)\n",
    "        junk_rate = junk/(positive+negative+junk)\n",
    "        analysis.loc[sample] = [sample,positive,negative,junk,efficiency,junk_rate]\n",
    "        print(sample,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,\n",
    "              '\\nefficiency: ',efficiency,'\\njunk rate: ',junk_rate,'\\n')\n",
    "        \n",
    "# write out to excel\n",
    "writer = pd.ExcelWriter('SR016-efficienciesd.xlsx', engine='xlsxwriter')\n",
    "analysis.to_excel(writer, sheet_name = 'PF020')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TW23 - PapRecT v. Beta (wiltype codons) in pARC8 with pJP RBS\n",
    "to check if the pJP RBS caused PapRecT to work better than Beta, which was seen in the SEER library screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TW23/trimmed_TW23-238_R1.fastq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0540c93bd9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mjunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrimmed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# quality filter any reads shorter than 50-nt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid alphabet, %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mas_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Map the file format to a sequence iterator:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_FormatToIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/Bio/File.py\u001b[0m in \u001b[0;36mas_handle\u001b[0;34m(handleish, mode, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandleish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TW23/trimmed_TW23-238_R1.fastq'"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['ssap','replicate','codon_counts'])\n",
    "file = 'TW23/trimmed_TW23-%s_R1.fastq'\n",
    "SSAPs = ['PapRecT','Beta']\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "\n",
    "s = 0\n",
    "rep = 1\n",
    "for j in range(238,244):\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % (str(j).zfill(2))\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [SSAPs[s],rep,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',SSAPs[s],'-',\n",
    "          rep,'with',count,'reads','of which',junk,'are junk.')\n",
    "\n",
    "    rep += 1\n",
    "    if rep == 4:\n",
    "        rep = 1\n",
    "        s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR016_1 \n",
      "positive:  170 \n",
      "negative:  3989 \n",
      "junk:  15 \n",
      "efficiency:  0.0408752103871 \n",
      "junk rate:  0.00359367513177 \n",
      "\n",
      "SR016_2 \n",
      "positive:  89 \n",
      "negative:  3354 \n",
      "junk:  12 \n",
      "efficiency:  0.0258495498112 \n",
      "junk rate:  0.00347322720695 \n",
      "\n",
      "SR016_3 \n",
      "positive:  758 \n",
      "negative:  17398 \n",
      "junk:  118 \n",
      "efficiency:  0.0417492839833 \n",
      "junk rate:  0.00645726168327 \n",
      "\n",
      "Beta_1 \n",
      "positive:  367 \n",
      "negative:  12499 \n",
      "junk:  79 \n",
      "efficiency:  0.0285247940308 \n",
      "junk rate:  0.00610274237157 \n",
      "\n",
      "Beta_2 \n",
      "positive:  376 \n",
      "negative:  9832 \n",
      "junk:  88 \n",
      "efficiency:  0.0368338557994 \n",
      "junk rate:  0.00854700854701 \n",
      "\n",
      "Beta_3 \n",
      "positive:  466 \n",
      "negative:  15709 \n",
      "junk:  94 \n",
      "efficiency:  0.0288098918083 \n",
      "junk rate:  0.00577785973323 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = pd.DataFrame(columns=['ssap','replicate','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "\n",
    "for ssap in tally['ssap'].unique():\n",
    "    data = tally[(tally['ssap'] == ssap)]\n",
    "    for replicate in data['replicate']:\n",
    "        for item in data[data['replicate'] == replicate].codon_counts:\n",
    "            positive = negative = junk = efficiency = junk_rate = 0\n",
    "            sample_name = ssap + '_' + str(replicate)\n",
    "            for i in item.index:\n",
    "                if i == editing_ref['negative']:\n",
    "                    negative = item[i]\n",
    "                elif i == editing_ref['positive']:\n",
    "                    positive = item[i]\n",
    "                else:\n",
    "                    junk += item[i]\n",
    "\n",
    "            if negative > 0 :\n",
    "                efficiency = positive/(positive+negative)\n",
    "            junk_rate = junk/(positive+negative+junk)\n",
    "            analysis.loc[sample_name] = [ssap,replicate,positive,negative,junk,efficiency,junk_rate]\n",
    "            print(sample_name,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,\n",
    "                  '\\nefficiency: ',efficiency,'\\njunk rate: ',junk_rate,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next3 - SR016 v. Beta (wiltype codons) and both in pARC8 with pJP RBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 seconds process time for loop SR016 - 1 with 21449 reads of which 1889 are junk.\n",
      "2.8 seconds process time for loop SR016 - 2 with 23730 reads of which 1937 are junk.\n",
      "2.8 seconds process time for loop SR016 - 3 with 24081 reads of which 2026 are junk.\n",
      "2.4 seconds process time for loop Beta - 1 with 21228 reads of which 2368 are junk.\n",
      "2.5 seconds process time for loop Beta - 2 with 22117 reads of which 2062 are junk.\n",
      "2.6 seconds process time for loop Beta - 3 with 22347 reads of which 1990 are junk.\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(columns=['ssap','replicate','codon_counts'])\n",
    "file = 'Next3/Data/trimmed/trimmed_S%i_merged_R1.fastq'\n",
    "SSAPs = ['SR016','Beta']\n",
    "pre_seq = 'TAGCTAAAAC'\n",
    "samples = [111,116]\n",
    "\n",
    "s = 0\n",
    "rep = 1\n",
    "for j in range(samples[0],samples[1]+1):\n",
    "    t0 = time.clock()\n",
    "\n",
    "    f = file % j\n",
    "    trimmed = SeqIO.parse(f,'fastq')\n",
    "\n",
    "    log = ['INIT']\n",
    "    count = 0\n",
    "    junk = 0\n",
    "    for r in trimmed:\n",
    "        # match to a 10-nt sequence before the edited region, allowing one mismatch or indel.\n",
    "        # quality filter any reads shorter than 50-nt.\n",
    "        seq = str(r.seq)\n",
    "        if len(seq) > 50:\n",
    "            match = find_near_matches(pre_seq,seq,max_l_dist=1)\n",
    "        if match:\n",
    "            log.append(seq[match[0][1]:match[0][1]+4])\n",
    "        else:\n",
    "            junk += 1\n",
    "\n",
    "        count += 1\n",
    "    df = pd.DataFrame(log)\n",
    "    df.columns = ['sequence']\n",
    "\n",
    "    tally.loc[len(tally)] = [SSAPs[s],rep,df.sequence.value_counts()]\n",
    "\n",
    "    print(round(time.clock() - t0,1),'seconds process time for loop',SSAPs[s],'-',\n",
    "          rep,'with',count,'reads','of which',junk,'are junk.')\n",
    "\n",
    "    rep += 1\n",
    "    if rep == 4:\n",
    "        rep = 1\n",
    "        s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR016_1 \n",
      "positive:  1411 \n",
      "negative:  18012 \n",
      "junk:  138 \n",
      "efficiency:  0.0726458322607 \n",
      "junk rate:  0.00705485404632 \n",
      "\n",
      "SR016_2 \n",
      "positive:  1719 \n",
      "negative:  19904 \n",
      "junk:  171 \n",
      "efficiency:  0.079498681959 \n",
      "junk rate:  0.00784619620079 \n",
      "\n",
      "SR016_3 \n",
      "positive:  1367 \n",
      "negative:  20531 \n",
      "junk:  158 \n",
      "efficiency:  0.0624257923098 \n",
      "junk rate:  0.00716358360537 \n",
      "\n",
      "Beta_1 \n",
      "positive:  611 \n",
      "negative:  18098 \n",
      "junk:  152 \n",
      "efficiency:  0.0326580789994 \n",
      "junk rate:  0.00805895763745 \n",
      "\n",
      "Beta_2 \n",
      "positive:  1023 \n",
      "negative:  18870 \n",
      "junk:  163 \n",
      "efficiency:  0.0514251244156 \n",
      "junk rate:  0.00812724371759 \n",
      "\n",
      "Beta_3 \n",
      "positive:  853 \n",
      "negative:  19339 \n",
      "junk:  166 \n",
      "efficiency:  0.0422444532488 \n",
      "junk rate:  0.0081540426368 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = pd.DataFrame(columns=['ssap','replicate','positive','negative','junk','efficiency','junk_rate'])\n",
    "editing_ref = {'positive':'CCCG','negative':'GCCG'}\n",
    "\n",
    "\n",
    "for ssap in tally['ssap'].unique():\n",
    "    data = tally[(tally['ssap'] == ssap)]\n",
    "    for replicate in data['replicate']:\n",
    "        for item in data[data['replicate'] == replicate].codon_counts:\n",
    "            positive = negative = junk = efficiency = junk_rate = 0\n",
    "            sample_name = ssap + '_' + str(replicate)\n",
    "            for i in item.index:\n",
    "                if i == editing_ref['negative']:\n",
    "                    negative = item[i]\n",
    "                elif i == editing_ref['positive']:\n",
    "                    positive = item[i]\n",
    "                else:\n",
    "                    junk += item[i]\n",
    "\n",
    "            if negative > 0 :\n",
    "                efficiency = positive/(positive+negative)\n",
    "            junk_rate = junk/(positive+negative+junk)\n",
    "            analysis.loc[sample_name] = [ssap,replicate,positive,negative,junk,efficiency,junk_rate]\n",
    "            print(sample_name,'\\npositive: ',positive,'\\nnegative: ',negative,'\\njunk: ',junk,\n",
    "                  '\\nefficiency: ',efficiency,'\\njunk rate: ',junk_rate,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
